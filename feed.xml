<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://matteogioia.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://matteogioia.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-26T15:55:11+00:00</updated><id>https://matteogioia.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal academic website of Matteo Gioia, PhD student in Data Science. </subtitle><entry><title type="html">1 - The Alignment Gap</title><link href="https://matteogioia.github.io/blog/2026/welcome/" rel="alternate" type="text/html" title="1 - The Alignment Gap"/><published>2026-02-26T00:00:00+00:00</published><updated>2026-02-26T00:00:00+00:00</updated><id>https://matteogioia.github.io/blog/2026/welcome</id><content type="html" xml:base="https://matteogioia.github.io/blog/2026/welcome/"><![CDATA[<h1 id="the-alignment-gap">The alignment gap</h1> <h3 id="introduction">Introduction</h3> <p>Welcome to the new and shiny blogpost section of this website! I decided to use this space as an informal personal diary, collecting resources for my future projects. For the sake of my sanity Iâ€™ll keep the blog simple, with minimal structuring and treating it as a flow of conscience space. Iâ€™ll also disable AI while writing - so any mistake is on me! ğŸ˜‰</p> <p>This first post discusses what I call the â€œAI alignment gapâ€. As of today we are starting to see the early signs of AI use diverging from its original goal. Instead of helping humanity to advance, we are running into the risk of creating systems that could behave against our interests, both willingly and unwillingly. We are constantly seeing new embodied autonomous agents being produced and platforms where they can collaborate with each other without supervision. The question is then natural: how can we ensure that we donâ€™t end up like Sarah Connor in Terminator? But more importantly, how do we take concrete and effective steps that go beyond just saying â€œAI is bad, we should limit its developmentâ€?</p> <p><img src="https://static0.srcdn.com/wordpress/wp-content/uploads/2019/11/Entry-3-Terminator-AI-Meme-from-imgflip.com-.jpg?q=50&amp;fit=crop&amp;w=825&amp;dpr=1.5" alt="https://screenrant.com/terminator-logic-memes-funny/"/></p> <h3 id="the-starting-point---what-we-have">The starting point - what we have</h3> <p>When I first started to reflect on this question I quickly realised I knew very little about AI safety as a whole. While I had recently co-authored a chapter on a Video and Motion Unlearning as a potential countermeasure (see <strong>TBA</strong>), my knowledge of the topic is still limited. As such, I opted to create this section to recap what I know, dig deeper into suggestions from colleagues (thanks Matteo M!) and keep track of resources I collect during this research.</p> <p><strong>The big voices</strong> â€” This paragraph discusses the work that was put out by people much smarter than I am in regards of creating actual AI safety guidelines. Of the top of my head, I can already mention is the AI act from the European Union.</p> <p><strong>The research community</strong> â€”</p> <p><strong>A non research oriented approach</strong> â€”</p> <p><strong>The bigger outlook</strong> â€”</p>]]></content><author><name></name></author><category term="updates"/><category term="general"/><summary type="html"><![CDATA[My bet on the biggest challenge of the next decade.]]></summary></entry></feed>